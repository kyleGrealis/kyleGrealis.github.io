[
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Kyle Grealis",
    "section": "",
    "text": "I’m a Biostatistician and Lead Data Analyst at the University of Miami. I spent nearly 20 years as a respiratory therapist before discovering the world of data science and its importance on healthcare. Attention to detail is as critical a skill in bedside care as it is in data analysis.\nI’ve been fortunate to work with some great physicians and researchers in the field of pulmonary medicine. Since the COVID-19 pandemic, I have transitioned to the Department of Public Health Sciences at the University of Miami. Currently, we are developing machine learning algorithms to predict individual relapse in those affected by the opioid epidemic.\nI’m excited about data science and the potential it has to improve healthcare and outcomes. This website is a collection of my work: R packages & web applications that I’ve developed to improve my day-to-day processes and contribute to the open source community. Let’s come together to grow the fields of data science and healthcare."
  },
  {
    "objectID": "about/index.html#hey-there-im-kyle-grealis",
    "href": "about/index.html#hey-there-im-kyle-grealis",
    "title": "Kyle Grealis",
    "section": "",
    "text": "I’m a Biostatistician and Lead Data Analyst at the University of Miami. I spent nearly 20 years as a respiratory therapist before discovering the world of data science and its importance on healthcare. Attention to detail is as critical a skill in bedside care as it is in data analysis.\nI’ve been fortunate to work with some great physicians and researchers in the field of pulmonary medicine. Since the COVID-19 pandemic, I have transitioned to the Department of Public Health Sciences at the University of Miami. Currently, we are developing machine learning algorithms to predict individual relapse in those affected by the opioid epidemic.\nI’m excited about data science and the potential it has to improve healthcare and outcomes. This website is a collection of my work: R packages & web applications that I’ve developed to improve my day-to-day processes and contribute to the open source community. Let’s come together to grow the fields of data science and healthcare."
  },
  {
    "objectID": "blog/posts/2024-02-15-how-to-scrape-google-scholar.html",
    "href": "blog/posts/2024-02-15-how-to-scrape-google-scholar.html",
    "title": "How to scrape Google Scholar using {httr2}",
    "section": "",
    "text": "Intro\nThis is a walkthrough on webscraping Google Scholar using SerpApi. This guide will show how to obtain a free API key allowing you to gather author publications and other information from Google Scholar. We’ll also go over how to securely store the API key in a .env file and how to access the key from the .env file.\n\n\n\nPrerequisites\nNavigate to the SerpApi website and register for a free account. You can sign in using your email or GitHub. After you register and confirm your email address, you will receive a secret key located towards the bottom of the screen.\n\n\n\nSerpApi key\n\n\nCreate a .env file in your project repo. You’ll want to immediately add that to your .gitignore file before you even commit the .env file to your repo. You do not want to make the mistake of scripting your secret key into your files or forget to do it later. In the .env file, add the following line:\n# my google scholar api key\nGOOGLE_KEY=\"&lt;your_secret_key_provided_when_you_registered&gt;\"\n\n\n\n\n\n\n\nNote\n\n\n\nEnd your .env file with a newline. This will save you a minor warning later on.\n\n\nNow you should save the file and commit it to your project repo.\n\n** Stay tuned… coming back with more! **"
  },
  {
    "objectID": "blog/posts/2024-04-19-KNN-in-python.html",
    "href": "blog/posts/2024-04-19-KNN-in-python.html",
    "title": "Machine Learning in Python - KNN",
    "section": "",
    "text": "In this post, I’m sharing the code that was created from following Kirill Eremenko and the SuperDataScience Team’s “Machine Learning A-Z” course on Udemy.\nThe prediction scenario is this: which demographic would social networking marketing ads affect best? We work for a car dealership and have data regarding consumers’ age and estimated salary. To where should marketing efforts be aimed as we try to predict which consumers will purchase our newest & best SUV model?\n\nEuclidean Distance between two points: \\(\\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\\)\nk-Nearest Neighbors (KNN) is machine learning technique used to classify a new data point to a nearby cluster. We will set up our algorithm to calculate the Euclidean distances from our new data point to existing data points. Then, using the predetermined k number of nearest neighbors (we’ll be using 5 neighbors), assign that new point to the closest cluster with at least three like \\(&gt;k/2\\) neighbors.\nTo think of it in really simple terms, all of our existing customers (points) are scattered in the \\((x, y)\\) space. Our new customer has \\(x\\) age and \\(y\\) salary, so we’ll plot this onto our existing grid. Then we draw circles around the new customer until we hit the closest existing point – that’s one “neighbor”. We repeat the process until we have our chosen number of 5. How many of the neighbors purchased the SUV and how many did not? Whichever group has more, that’s what we’re going to predict the new customer would do too!\n\n\n\nCourtesy of DataCamp"
  },
  {
    "objectID": "blog/posts/2024-04-19-KNN-in-python.html#what-is-knn",
    "href": "blog/posts/2024-04-19-KNN-in-python.html#what-is-knn",
    "title": "Machine Learning in Python - KNN",
    "section": "",
    "text": "In this post, I’m sharing the code that was created from following Kirill Eremenko and the SuperDataScience Team’s “Machine Learning A-Z” course on Udemy.\nThe prediction scenario is this: which demographic would social networking marketing ads affect best? We work for a car dealership and have data regarding consumers’ age and estimated salary. To where should marketing efforts be aimed as we try to predict which consumers will purchase our newest & best SUV model?\n\nEuclidean Distance between two points: \\(\\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\\)\nk-Nearest Neighbors (KNN) is machine learning technique used to classify a new data point to a nearby cluster. We will set up our algorithm to calculate the Euclidean distances from our new data point to existing data points. Then, using the predetermined k number of nearest neighbors (we’ll be using 5 neighbors), assign that new point to the closest cluster with at least three like \\(&gt;k/2\\) neighbors.\nTo think of it in really simple terms, all of our existing customers (points) are scattered in the \\((x, y)\\) space. Our new customer has \\(x\\) age and \\(y\\) salary, so we’ll plot this onto our existing grid. Then we draw circles around the new customer until we hit the closest existing point – that’s one “neighbor”. We repeat the process until we have our chosen number of 5. How many of the neighbors purchased the SUV and how many did not? Whichever group has more, that’s what we’re going to predict the new customer would do too!\n\n\n\nCourtesy of DataCamp"
  },
  {
    "objectID": "blog/posts/2024-04-19-KNN-in-python.html#import-libraries",
    "href": "blog/posts/2024-04-19-KNN-in-python.html#import-libraries",
    "title": "Machine Learning in Python - KNN",
    "section": "Import libraries",
    "text": "Import libraries\n\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "blog/posts/2024-04-19-KNN-in-python.html#import-dataset",
    "href": "blog/posts/2024-04-19-KNN-in-python.html#import-dataset",
    "title": "Machine Learning in Python - KNN",
    "section": "Import dataset",
    "text": "Import dataset\n\ndataset = pd.read_csv('data/Social_Network_Ads.csv')\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values"
  },
  {
    "objectID": "blog/posts/2024-04-19-KNN-in-python.html#splitting-the-dataset-to-training-testing",
    "href": "blog/posts/2024-04-19-KNN-in-python.html#splitting-the-dataset-to-training-testing",
    "title": "Machine Learning in Python - KNN",
    "section": "Splitting the dataset to training & testing",
    "text": "Splitting the dataset to training & testing\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n  X, y,\n  test_size=0.25,\n  random_state=0\n)"
  },
  {
    "objectID": "blog/posts/2024-04-19-KNN-in-python.html#feature-scaling",
    "href": "blog/posts/2024-04-19-KNN-in-python.html#feature-scaling",
    "title": "Machine Learning in Python - KNN",
    "section": "Feature scaling",
    "text": "Feature scaling\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\n\n```\n# array([[-1.455..., -0.784...],\n#        [ 2.067...,  1.372...],\n#        [-0.253..., -0.309...],\n#        ...,\n#        [-0.253..., -0.309...],\n#        [ 2.067..., -1.113...],\n#        [-1.455..., -0.309...]])\n```"
  },
  {
    "objectID": "blog/posts/2024-04-19-KNN-in-python.html#train-fit-the-knn-model",
    "href": "blog/posts/2024-04-19-KNN-in-python.html#train-fit-the-knn-model",
    "title": "Machine Learning in Python - KNN",
    "section": "Train & fit the KNN model",
    "text": "Train & fit the KNN model\n\n\n\n\n\n\nTip\n\n\n\nTo learn the more technical details of sklearn’s classes and functions, checkout the sklearn API Reference.\n\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(\n  n_neighbors=5,  # default\n  p=2,            # euclidean distance; default \n  metric='minkowski'\n)\n\n\nclassifier.fit(X_train, y_train)"
  },
  {
    "objectID": "blog/posts/2024-04-19-KNN-in-python.html#predicting-a-new-result",
    "href": "blog/posts/2024-04-19-KNN-in-python.html#predicting-a-new-result",
    "title": "Machine Learning in Python - KNN",
    "section": "Predicting a new result",
    "text": "Predicting a new result\n30y/o $87k/yr – first observation of X_test\n\nperson = X_test[[0]]\nsingle_pred = classifier.predict(person)\nsingle_prob = classifier.predict_proba(person)\nprint('1=\"Yes\", 0=\"No\"\\n')\nprint(f'Single prediction for 30 y/o earning $87k/yr: {single_pred[0]} at a probability of {single_prob[0][0].round(3)}')\n\n```\n# 1=\"Yes\", 0=\"No\"\n#\n# Single prediction for 30 y/o earning $87k/yr: 0 at a \n# probability of 0.8\n```"
  },
  {
    "objectID": "blog/posts/2024-04-19-KNN-in-python.html#predicting-the-test-set-results",
    "href": "blog/posts/2024-04-19-KNN-in-python.html#predicting-the-test-set-results",
    "title": "Machine Learning in Python - KNN",
    "section": "Predicting the test set results",
    "text": "Predicting the test set results\n\ny_pred = classifier.predict(X_test)"
  },
  {
    "objectID": "blog/posts/2024-04-19-KNN-in-python.html#creating-the-confusion-matrix",
    "href": "blog/posts/2024-04-19-KNN-in-python.html#creating-the-confusion-matrix",
    "title": "Machine Learning in Python - KNN",
    "section": "Creating the confusion matrix",
    "text": "Creating the confusion matrix\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nprint(confusion_matrix(y_test, y_pred))\nprint(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n\n```\n# [[64  4]\n#  [ 3 29]]\n# Accuracy: 0.93\n```"
  },
  {
    "objectID": "blog/posts/2024-04-19-KNN-in-python.html#visualizing-the-training-set-results",
    "href": "blog/posts/2024-04-19-KNN-in-python.html#visualizing-the-training-set-results",
    "title": "Machine Learning in Python - KNN",
    "section": "Visualizing the training set results",
    "text": "Visualizing the training set results\n\n\n\n\n\n\nWarning\n\n\n\nThese next two code chunks will take a while. The KNN algorithm is already compute-expensive and we’re adding to the heavy lifting by creating a grid of many values to be calculated. The final result is two plots with a visual mapping of our decision boundary and our training & predicted data points appearing as the dots within the field.\n\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nX_set, y_set = sc.inverse_transform(X_train), y_train\nX1, X2 = np.meshgrid(\n  np.arange(\n    start = X_set[:, 0].min() - 10, \n    stop = X_set[:, 0].max() + 10, \n    step = 0.25\n  ),\n  np.arange(\n    start = X_set[:, 1].min() - 1000, \n    stop = X_set[:, 1].max() + 1000, \n    step = 0.25\n  )\n)\nplt.contourf(\n  X1, X2, \n  classifier.predict(\n    sc.transform(np.array([X1.ravel(), X2.ravel()]).T)\n  ).reshape(X1.shape),\n  alpha = 0.75, \n  cmap = ListedColormap(('red', 'green'))\n)\n\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\n\nfor i, j in enumerate(np.unique(y_set)):\n  plt.scatter(\n    X_set[y_set == j, 0], \n    X_set[y_set == j, 1], \n    c = ListedColormap(('red', 'green'))(i), \n    label = j\n  )\n\nplt.title('KNN Regression (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\n\n\n\nKNN training set results"
  },
  {
    "objectID": "blog/posts/2024-04-19-KNN-in-python.html#visualizing-the-test-set-results",
    "href": "blog/posts/2024-04-19-KNN-in-python.html#visualizing-the-test-set-results",
    "title": "Machine Learning in Python - KNN",
    "section": "Visualizing the test set results",
    "text": "Visualizing the test set results\n\nX_set, y_set = sc.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(\n  np.arange(\n    start = X_set[:, 0].min() - 10, \n    stop = X_set[:, 0].max() + 10, \n    step = 0.25\n  ),\n  np.arange(\n    start = X_set[:, 1].min() - 1000, \n    stop = X_set[:, 1].max() + 1000, \n    step = 0.25\n  )\n)\nplt.contourf(\n  X1, X2, \n  classifier.predict(\n    sc.transform(np.array([X1.ravel(), X2.ravel()]).T)\n  ).reshape(X1.shape),\n  alpha = 0.75, \n  cmap = ListedColormap(('red', 'green'))\n)\n\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\n\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(\n      X_set[y_set == j, 0], \n      X_set[y_set == j, 1], \n      c = ListedColormap(('red', 'green'))(i), \n      label = j\n    )\n\nplt.title('KNN Regression (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\n\n\n\nKNN testing set results\n\n\nShare your insights at kylegrealis@icloud.com. Together, we can make our R projects more robust, reproducible, and ready for collaboration!\nHappy coding!\n~Kyle"
  },
  {
    "objectID": "blog/posts/2025-04-17-github-teams.html#using-a-collaborative-git-workflow-part-2",
    "href": "blog/posts/2025-04-17-github-teams.html#using-a-collaborative-git-workflow-part-2",
    "title": "UPDATE: Collaborative Git Workflow",
    "section": "Using a collaborative git workflow, part 2",
    "text": "Using a collaborative git workflow, part 2\nWelcome to my blog post about effective team collaboration for R package development! As someone who’s spent time developing R packages for personal use & in academic settings, I’ve learned that a clear Git workflow is essential for smooth collaboration. Having this understanding is just as advantageous for team development as it is for working by yourself.\nThis guide outlines how our team manages development for our R packages, with multiple contributors working in parallel while maintaining a stable codebase. We use a branching strategy that keeps our CRAN releases pristine while allowing active development to continue.\nOur team follows this workflow with a main branch containing only stable CRAN-approved versions, a dev branch serving as the base for future improvements, and feature branches for individual enhancements. This approach has proven effective for small academic teams (3-4 people) collaborating on R packages for machine learning and statistics.\nI will be referencing my preferred method of using bash commands used in the terminal, though you can incorporate this into your IDE-specific workflow too. I have also included some RStudio guidelines along the way.\n\nThis is an update to last year’s post: Quick git for beginners… yeah, that’s me!. 11 months later and I still find plenty of valuable tricks along the way.\n\n\n\n\n\n\n\nTerms to understand:\n\n\n\nlocal: The project version that is on your working machine.\norigin: The project version on GitHub, for example."
  },
  {
    "objectID": "blog/posts/2025-04-17-github-teams.html#our-branch-structure",
    "href": "blog/posts/2025-04-17-github-teams.html#our-branch-structure",
    "title": "UPDATE: Collaborative Git Workflow",
    "section": "Our Branch Structure",
    "text": "Our Branch Structure\n\nmain: Contains only stable, CRAN-approved versions\ndev: Our primary development branch where we integrate completed features\nFeature branches: Individual branches created from dev for specific features/improvements"
  },
  {
    "objectID": "blog/posts/2025-04-17-github-teams.html#recommended-workflow",
    "href": "blog/posts/2025-04-17-github-teams.html#recommended-workflow",
    "title": "UPDATE: Collaborative Git Workflow",
    "section": "Recommended Workflow",
    "text": "Recommended Workflow\n\n1. Starting Work on a New Feature\n# Make sure you're on dev and it's up-to-date\ngit switch dev\ngit pull origin dev\n\n# Create your feature branch\ngit switch -c feature-name\n\n\n2. Regular Maintenance While Working on Your Feature\nIf others have merged changes to dev while you’re working on your feature:\n# Make sure your changes are committed locally first\ngit add .\ngit commit -m \"Descriptive message of your current work\"\n\n# Get the latest changes from remote\ngit fetch origin\n\n# Incorporate latest dev changes into your branch\ngit merge origin/dev\n\n# Resolve any conflicts if they arise\n# Test that everything still works correctly\nYou should do this periodically (weekly or after major changes to dev).\n\n\n3. Completing Your Feature\n# Final update from dev before submitting\ngit switch feature-name\ngit fetch origin\ngit merge origin/dev\n# Run R tests to ensure everything works\ndevtools::check()\npkgdown::build_site()  # if applicable\n# Push your feature branch to remote\ngit push origin feature-name\n\n# Create a pull request from feature-name to dev\n# (through GitHub/GitLab interface)"
  },
  {
    "objectID": "blog/posts/2025-04-17-github-teams.html#why-we-use-merge-vs.-rebase",
    "href": "blog/posts/2025-04-17-github-teams.html#why-we-use-merge-vs.-rebase",
    "title": "UPDATE: Collaborative Git Workflow",
    "section": "Why We Use Merge vs. Rebase",
    "text": "Why We Use Merge vs. Rebase\nWe recommend using merge rather than rebase for our academic team because:\n\nSafer for collaboration: Merge preserves the exact history of what happened and when\nMore straightforward conflict handling: With merge, you resolve conflicts just once at the point of integration, while rebase might require resolving the same conflict multiple times as it replays each of your commits\nEasier to understand: The merge commit clearly shows when code from dev was integrated\nLess risky: Rebase rewrites commit history, which can cause issues when multiple people work on the same branch"
  },
  {
    "objectID": "blog/posts/2025-04-17-github-teams.html#git-commands-explained",
    "href": "blog/posts/2025-04-17-github-teams.html#git-commands-explained",
    "title": "UPDATE: Collaborative Git Workflow",
    "section": "Git Commands Explained",
    "text": "Git Commands Explained\n\n\n\n\n\n\n\nCommand\nPlain Language Explanation\n\n\n\n\ngit fetch origin\n“Check what changes have been made on GitHub since I last synced. This updates my local copy of what exists on GitHub, but doesn’t actually change my working files yet.”\n\n\ngit merge origin/dev\n“Take all the new commits that exist in the dev branch and add them to my current branch, creating a new ‘merge commit’ that joins the two branches’ histories.”\n\n\ngit pull\nShortcut that combines fetch + merge for your current branch. Equivalent to git pull origin &lt;current-branch&gt;\n\n\ngit pull origin dev\n“Fetch from GitHub and merge the remote dev branch into my current branch”"
  },
  {
    "objectID": "blog/posts/2025-04-17-github-teams.html#understanding-fetch-vs.-pull",
    "href": "blog/posts/2025-04-17-github-teams.html#understanding-fetch-vs.-pull",
    "title": "UPDATE: Collaborative Git Workflow",
    "section": "Understanding Fetch vs. Pull",
    "text": "Understanding Fetch vs. Pull\n\ngit fetch only downloads new data from GitHub but doesn’t integrate it into your working files. It’s like checking what’s new without actually applying those changes.\nBenefits of fetch before merge:\n\nYou can examine what changed before deciding to merge\nYou can prepare for potential conflicts\nYou can run git diff origin/dev to see exactly what will change\nIt’s safer when you’re in the middle of complex work and want to know what’s coming\n\ngit pull immediately fetches AND merges in one step. It’s convenient but gives you less control.\n\nFor an academic team, the fetch then merge approach gives you time to prepare for integration and is less likely to introduce unexpected conflicts in the middle of your work."
  },
  {
    "objectID": "blog/posts/2025-04-17-github-teams.html#resolving-conflicts",
    "href": "blog/posts/2025-04-17-github-teams.html#resolving-conflicts",
    "title": "UPDATE: Collaborative Git Workflow",
    "section": "Resolving Conflicts",
    "text": "Resolving Conflicts\nConflicts occur when the same part of a file has been modified differently in both branches being merged. Here are recommended ways to resolve them:\n\nUsing GitHub (Web Interface)\n\nWhen a pull request has conflicts, GitHub shows a “Resolve conflicts” button\nThis opens a text editor where you can edit the conflicted files\nRemove the conflict markers (&lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, &gt;&gt;&gt;&gt;&gt;&gt;&gt;) and keep the code you want\nClick “Mark as resolved” for each file, then “Commit merge”\n\n\n\nUsing RStudio IDE\n\nWhen conflicts occur, files with conflicts will be marked in the Git pane\nClick on the file to open the diff view\nFor each conflict section, choose “Use me” (your changes) or “Use them” (incoming changes), or edit the code manually\nSave the file, then stage it (check the box in the Git pane)\nComplete the merge by committing\n\n\n\nUsing Command Line\n# After git merge shows conflicts, you'll see a message identifying which files have conflicts\n# You MUST open these files in your preferred text editor to see and resolve the conflicts\n# The conflicts are marked with &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt; markers\n\n# For example, open a conflicted file:\nopen conflicted_file.R\n\n# After editing and resolving all conflicts in all files, mark them as resolved:\ngit add &lt;resolved-file&gt;\n\n# Once all conflicts are resolved and added:\ngit commit -m \"Merge dev into feature-branch, resolving conflicts\""
  },
  {
    "objectID": "blog/posts/2025-04-17-github-teams.html#common-scenarios",
    "href": "blog/posts/2025-04-17-github-teams.html#common-scenarios",
    "title": "UPDATE: Collaborative Git Workflow",
    "section": "Common Scenarios",
    "text": "Common Scenarios\n\nScenario: dev has changed significantly while you were working\ngit switch your-feature-branch\ngit fetch origin\ngit merge origin/dev\n# Resolve conflicts if any\n# Run tests to make sure everything still works\n\n\nScenario: Multiple team members working on the same feature\n# Before starting work each day\ngit switch feature-branch\ngit pull origin feature-branch\n\n# After finishing for the day\ngit push origin feature-branch\n\n\nScenario: Need to temporarily switch to another task\n# Save your current work without committing\ngit stash\n\n# Switch to another branch\ngit switch other-branch\n\n# Do your work, commit, etc.\n# ...\n\n# Go back to your original branch\ngit switch feature-branch\n\n# Restore your work\ngit stash pop\n\n\nScenario: Two people modified the same file in the same branch\nThis is a common situation, especially when team communication isn’t perfect:\n\nJoe and Maria are both working on the same feature branch\nJoe modifies the plot_results.R file, commits, and pushes to GitHub\nMeanwhile, Maria has also modified plot_results.R locally (different changes)\nWhen Maria tries to push her changes, she gets an error:\n\n! [rejected] feature-branch -&gt; feature-branch (fetch first)\nerror: failed to push some refs\nWhat happened? GitHub rejected Maria’s push because her local branch has diverged from the remote branch. She needs to integrate Joe’s changes before she can push her own.\nHow to resolve this:\n# First, get Joe's changes\ngit pull origin feature-branch\n\n# Git will attempt to auto-merge the changes\n# If there are conflicts, Git will tell you which files are affected\n\n# Open each conflicted file in your editor\n# You'll see conflict markers like:\n# &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD (your changes)\n# function(x) { return(x^2) }\n# =======\n# function(x) { return(x^2 + 1) }\n# &gt;&gt;&gt;&gt;&gt;&gt;&gt; (Joe's changes)\n\n# Edit the file to resolve conflicts\n# Remove the conflict markers and keep the correct code\n\n# After editing all conflicts:\ngit add plot_results.R\ngit commit -m \"Merge Joe's changes with mine\"\ngit push origin feature-branch\n\n\n\n\n\n\nHelpful tip:\n\n\n\nThis situation highlights why it’s crucial to communicate with teammates about who’s working on what. Consider using GitHub issues to track who’s assigned to each task, or at minimum, send a quick message to teammates when you start working on a file that others might also be editing."
  },
  {
    "objectID": "blog/posts/2025-04-17-github-teams.html#best-practices-for-r-package-development",
    "href": "blog/posts/2025-04-17-github-teams.html#best-practices-for-r-package-development",
    "title": "UPDATE: Collaborative Git Workflow",
    "section": "Best Practices for R Package Development",
    "text": "Best Practices for R Package Development\n\nAlways update roxygen documentation when modifying functions\nRun tests after merging in changes from dev\nUse meaningful commit messages that explain why a change was made\nConsider using GitHub Issues to track features and bugs\nReview each other’s pull requests before merging to dev\nWhen updating a function, also update the corresponding unit tests\nDocument all changes in NEWS.md"
  },
  {
    "objectID": "blog/posts/2025-04-17-github-teams.html#common-pitfalls-to-avoid",
    "href": "blog/posts/2025-04-17-github-teams.html#common-pitfalls-to-avoid",
    "title": "UPDATE: Collaborative Git Workflow",
    "section": "Common Pitfalls to Avoid",
    "text": "Common Pitfalls to Avoid\n\nMerging without testing: Always test after merging to ensure your code still works\nLong-lived or stale branches: Try to keep feature branches short-lived (1-2 weeks max)\nInfrequent merges from dev: Update regularly to avoid massive conflicts later\nPushing broken code: Always run devtools::check() before pushing to shared branches\nAmbiguous commit messages: Use clear messages that explain the “why” not just the “what”\n\nShare your insights at kylegrealis@icloud.com. Together, we can make our R projects more robust, reproducible, and ready for collaboration!\nHappy coding!\n~Kyle"
  },
  {
    "objectID": "blog/posts/2025-04-17-github-teams.html#additional-resources",
    "href": "blog/posts/2025-04-17-github-teams.html#additional-resources",
    "title": "UPDATE: Collaborative Git Workflow",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nR Packages (2e) by Hadley Wickham & Jenny Bryan\nHappy Git with R by Jenny Bryan\nGit for Teams by Emma Jane Hogbin Westby"
  },
  {
    "objectID": "blog/posts/2025-04-22-the-why.html",
    "href": "blog/posts/2025-04-22-the-why.html",
    "title": "Home Lab, part 0: The “Why”?",
    "section": "",
    "text": "Steps not covered:"
  },
  {
    "objectID": "blog/posts/2025-04-22-the-why.html#purpose-of-my-home-lab",
    "href": "blog/posts/2025-04-22-the-why.html#purpose-of-my-home-lab",
    "title": "Home Lab, part 0: The “Why”?",
    "section": "Purpose of my Home Lab",
    "text": "Purpose of my Home Lab\nThe origin of this project is hopefully relatable to you, the reader: I just wanted to see if I could just do it. This question though was: am I capable of deploying self-hosted services on my home network, and then safely & securely access them from outside the home?\nNetworking? Never heard of her, well not insofar as an academic data analyst. Our team tends to operate more in GitHub collaborative projects (thus my potential over-posting of GitHub things here, here and especially here). I just wanted to geek out, rsync my Timeshift backups and other files off my laptop to another device, and feel like one of the cool tech kids.\n\n\n\n\n\n\nUPDATE:\n\n\n\nI’m still not a cool kid, but got this working so there you go!\n\n\nThe main goals I established before undertaking this project were:\n\nLearn to remotely access devices on my home network\nShare files across devices\nSet-up a Raspberry Pi for self-hosted services\nLearn “WTH is Docker and how do I use it?”\nMake it so that only I can access these on a secure connection! No connection, no access.\nTest how much I can de-Google & de-Apple my life\n\nThe last part involved needing to find ways to self-host document & file management and photos. The cost of using cloud storage isn’t all that intrusive, but it does add up over time. One major hurdle would be leaving the comfort of known apps and have it all fitting easily into daily life.\nHappily, with help from r/selfhosted and some feedback from YouTube, I’ve been able to tweak things just enough to make it work for me. I should have stated this towards the top, but I’ve made plenty of mistakes and had a few moments of “oh $h!t… did I just brick my system, lose my data, erase x, y, & z??” but…\nLive, laugh, and backup all your important things first!\n\nShare your insights at kylegrealis@icloud.com. Together, we can make our R projects more robust, reproducible, and ready for collaboration!\nHappy coding!\n~Kyle"
  },
  {
    "objectID": "blog/posts/2024-08-19-renv-project-sharing-guide.html#how-this-guide-can-be-helpful",
    "href": "blog/posts/2024-08-19-renv-project-sharing-guide.html#how-this-guide-can-be-helpful",
    "title": "renv Project Setup and Sharing Guide",
    "section": "How this guide can be helpful",
    "text": "How this guide can be helpful\n\n\n\n\n\n\nWhile this guide was initially crafted for my team working on machine learning projects, the principles and steps outlined here are broadly applicable to any R-based data science project. Whether you’re using RStudio, Positron, or another IDE, you may need to slightly adapt these instructions to fit your specific setup. The core concepts, however, remain the same across environments. 😊\n\n\n\n\nIn the ever-evolving landscape of data science and machine learning, maintaining reproducible environments is crucial yet often overlooked. This renv project guide aims to streamline collaboration and ensure consistency across different systems and team members. It’s not an exhaustive technical manual, nor does it delve into every possible scenario you might encounter. Instead, consider this a practical starting point, peppered with real-world tips I’ve gathered from navigating the choppy waters of project management and collaboration. My goal? To help you focus on what truly matters: unleashing your data science superpowers without getting bogged down by environment inconsistencies! Whether you’re a solo data explorer or part of a larger team, these steps will set you on the path to smoother, more reproducible R projects."
  },
  {
    "objectID": "blog/posts/2024-08-19-renv-project-sharing-guide.html#project-structure",
    "href": "blog/posts/2024-08-19-renv-project-sharing-guide.html#project-structure",
    "title": "renv Project Setup and Sharing Guide",
    "section": "Project Structure",
    "text": "Project Structure\nproject_root/\n├── .Rproj.user/\n├── renv/\n│   ├── activate.R\n│   ├── library/\n│   ├── settings.json\n│   └── .gitignore\n├── scripts/\n│   ├── script1.R\n│   ├── script2.R\n│   ├── script3.R\n│   ├── script4.R\n│   └── script5.R\n├── .Rprofile\n├── .gitignore\n├── project_name.Rproj\n└── renv.lock"
  },
  {
    "objectID": "blog/posts/2024-08-19-renv-project-sharing-guide.html#for-the-project-creator",
    "href": "blog/posts/2024-08-19-renv-project-sharing-guide.html#for-the-project-creator",
    "title": "renv Project Setup and Sharing Guide",
    "section": "For the Project Creator",
    "text": "For the Project Creator\n\nInitialize the project:\n\nCreate a new R project or navigate to an existing project directory.\nOpen the project in RStudio or Positron.\n\nSet up renv:\n\nRun install.packages(\"renv\") if not already installed.\nInitialize renv by running:\nrenv::init()\n\nDevelop your project:\n\nCreate your R scripts (in this case, 5 main scripts).\nUse renv::install(\"package_name\") to add new packages as needed.\n\nCapture the project state:\n\nAfter finalizing your scripts, run:\nrenv::snapshot()\nThis creates/updates the renv.lock file with your project’s dependencies.\n\nReview the lockfile:\n\nOpen renv.lock and ensure all necessary packages are included.\nIf any are missing, install them with renv::install() and run renv::snapshot() again.\n\nPrepare for sharing:\n\nEnsure your project directory contains:\n\nAll R scripts\n.Rproj file (not included if the project was created within Positron, so not needed)\nrenv.lock file\n.Rprofile file (if present)\nrenv/ directory\n\n\nShare the project:\n\nCompress the entire project directory.\nSend the compressed file to your collaborator.\n\n\n\nAlternative option\nIf you have been working on non-renv activated project while conducting exploratory data analysis (EDA) or just tinkering with machine learning preprocessing recipes and modeling types, you can activate renv after your project is in a steady state.\n\nSet up renv:\n\nRun install.packages(\"renv\") if not already installed.\nInitialize renv by running:\nrenv::init()\nThis will analyze your existing project and create an initial lockfile based on your current package usage. It examines the scripts you’ve written for the presence of any library() or require() calls and adds those packages.\n\nCapture the project state:\n\nAfter finalizing your scripts, run:\nrenv::snapshot()\n\nReview the lockfile:\n\nOpen renv.lock and ensure all necessary packages are included.\nNOTE: Using the package::function() method may cause renv::snapshot() to miss these packages.\nIf any are missing, install them with renv::install() and run renv::snapshot() again.\n\n\nProceed to share your project in its working, ready-to-share state."
  },
  {
    "objectID": "blog/posts/2024-08-19-renv-project-sharing-guide.html#for-the-new-user",
    "href": "blog/posts/2024-08-19-renv-project-sharing-guide.html#for-the-new-user",
    "title": "renv Project Setup and Sharing Guide",
    "section": "For the New User",
    "text": "For the New User\n\nReceive and extract the project:\n\nReceive the compressed project directory from the creator.\nExtract it to a desired location on your machine.\n\nSet up R environment:\n\nInstall R and RStudio if not already installed.\nInstall renv by opening R/RStudio and running:\ninstall.packages(\"renv\")\n\nOpen the project:\n\nNavigate to the extracted project directory.\nDouble-click the .Rproj file to open the project in RStudio.\n\nRestore the project environment:\n\nIn the R console, run:\nrenv::restore()\nThis will install all necessary packages as specified in the renv.lock file.\n\nVerify the setup:\n\nTry running one of the scripts to ensure everything is working correctly.\n\nStart working:\n\nYou now have an exact replica of the original project environment.\nIf you need to add new packages, use renv::install() followed by renv::snapshot()."
  },
  {
    "objectID": "blog/posts/2024-08-19-renv-project-sharing-guide.html#notes",
    "href": "blog/posts/2024-08-19-renv-project-sharing-guide.html#notes",
    "title": "renv Project Setup and Sharing Guide",
    "section": "Notes",
    "text": "Notes\n\nAlways use renv::install() instead of install.packages() when adding new packages to an renv project.\nRun renv::status() periodically to check if your lockfile is in sync with your current project state.\nIf you encounter issues, try running renv::repair() to fix inconsistencies.\n\nBy following these steps, both the project creator and the new user ensure a consistent, reproducible environment for the R project.\n\nI hope this renv project guide proves valuable in streamlining your R project workflows and enhancing collaboration within your team. As someone who’s grappled with dependency management headaches, I’ve found that testing these renv practices on personal projects before introducing them to the team can save a lot of collective frustration. Remember, using renv isn’t just about managing packages - it’s about creating a shared, reproducible environment that speaks to the core of collaborative data science.\nWhen working with renv, think of your renv.lock file as a crucial piece of documentation: it tells the story of your project’s exact environment. Just as you’d write clear git commit messages (not just “updated stuff” but “feat: added tidymodels for ML pipeline”), be intentional about when and why you update your lockfile. A well-maintained renv setup can be as informative as well-commented code!\nThis guide is, of course, a work in progress. As I explore more advanced renv features or discover new best practices in R project management, I’ll update this post accordingly. If you’ve found clever ways to use renv in your data science projects or have tips for smoother collaboration using these tools, I’m all ears!\nShare your insights at kylegrealis@icloud.com. Together, we can make our R projects more robust, reproducible, and ready for collaboration!\nHappy coding!\n~Kyle"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kyle Grealis",
    "section": "",
    "text": "I’m a Biostatistician and Lead Data Analyst at the University of Miami. After nearly 20 years as a respiratory therapist, I discovered data science and its potential to transform healthcare. The attention to detail that’s essential in bedside care translates directly to data analysis.\nI’ve worked with physicians and researchers in pulmonary medicine, and since COVID-19, I’ve transitioned to the Department of Public Health Sciences. Currently, we’re developing machine learning algorithms to predict individual relapse in those affected by the opioid epidemic.\nI’m passionate about open source software and have authored the {froggeR} and {nascaR.data} R packages, co-authored {rUM}, and contributed to {tidyREDCap} and other packages. This website showcases my R packages and web applications that streamline workflows and contribute to the community.\nWhen I’m not analyzing data, you’ll find me following track & field, running, or tinkering with Linux systems."
  },
  {
    "objectID": "index.html#hey-there-im-kyle-grealis",
    "href": "index.html#hey-there-im-kyle-grealis",
    "title": "Kyle Grealis",
    "section": "",
    "text": "I’m a Biostatistician and Lead Data Analyst at the University of Miami. After nearly 20 years as a respiratory therapist, I discovered data science and its potential to transform healthcare. The attention to detail that’s essential in bedside care translates directly to data analysis.\nI’ve worked with physicians and researchers in pulmonary medicine, and since COVID-19, I’ve transitioned to the Department of Public Health Sciences. Currently, we’re developing machine learning algorithms to predict individual relapse in those affected by the opioid epidemic.\nI’m passionate about open source software and have authored the {froggeR} and {nascaR.data} R packages, co-authored {rUM}, and contributed to {tidyREDCap} and other packages. This website showcases my R packages and web applications that streamline workflows and contribute to the community.\nWhen I’m not analyzing data, you’ll find me following track & field, running, or tinkering with Linux systems."
  },
  {
    "objectID": "apps/index.html",
    "href": "apps/index.html",
    "title": "Web Applications",
    "section": "",
    "text": "PowerViz\nIntroducing PowerViz, an engaging Shiny app designed to bring power calculations to life! Originally crafted as a handy tool for a clinical trials class presentation, it has evolved into a comprehensive web application due to overwhelming positive feedback.\nPowerViz was born out of a need to illustrate the impact of sample size and effect size on power calculations. It’s a testament to the adage that necessity is the mother of invention. Built on the robust pwr R package, PowerViz offers an interactive platform where users can manipulate variables such as significance level, effect size, and sample size or proportion.\nBut PowerViz is more than just a tool; it’s an immersive learning experience. As you interact with the plots and adjust the inputs, you receive real-time feedback, providing valuable insights into the dynamics of power calculations. Whether you’re looking to enhance your learning or in the throes of designing a study, PowerViz is your go-to resource. Dive in and experience the fun side of statistics!\n\nTry it!\n\n\n\n\n\n\n\n\n\nEpiMatch\nEpiMatch is your indispensable partner for case-control matching and it’s now available as a Shiny web application. This tool is designed to streamline the matching process, strategically minimizing biases and saving you precious time. Case-control matching is a critical technique used in epidemiological studies to identify and analyze factors that may contribute to a medical condition or disease.\nEpiMatch empowers you to upload your own data and engage with a sophisticated matching algorithm, tailored to your specific criteria. While the tool comes with comprehensive instructions on its main page, it does require a touch of preprocessing before use.\nAfter running through five matching iterations, EpiMatch presents you with the results from the iteration that yielded the highest number of matched cases to controls. And the best part? Every table is downloadable, ensuring no data is lost, even for those individuals (cases and controls) that were not successfully matched.\nSo why wait? Dive into EpiMatch and experience the future of case-control matching!\n\nTry it!"
  },
  {
    "objectID": "r-packages/index.html",
    "href": "r-packages/index.html",
    "title": "R Packages",
    "section": "",
    "text": "froggeR\nfroggeR is an R package designed to streamline the creation and management of Quarto projects. It provides a suite of tools to automate setup, ensure consistency, and enhance collaboration in data science workflows.\n\nLeap ahead in your data science journey with froggeR! Streamline Quarto workflows, create structured projects, and enhance collaboration with ease. 🐸\n\nfroggeR simplifies project setup so you can focus on what matters:\n\nEfficiency: Minimize setup time, maximize analysis time\nConsistency: Uniform styling and structure across all your projects\nReliability: Prevent common setup issues before they occur\nSecurity: Robust .gitignore settings for enhanced data protection\nCollaboration: Structured documentation for seamless team onboarding\nCustomization: Easy-to-use tools for tailoring project aesthetics\nReproducibility: Ensure consistent environments across team members\n\n\nGet it now!\n\n\n\n\n\n\n\n\n\nnascaR.data\nnascaR.data provides historical race results from NASCAR’s top three series: Cup (1949-present), Xfinity (1982-present), and Trucks (1995-present). Explore driver, team, and manufacturer performance in a race-by-race, season, or career format. This data has been expertly curated and scraped with permission from DriverAverages.com.\n\nGet it now!\n\n\n\n\n\n\n\n\n\nrUM\nThis is a collection of R things from your friends at UM (The University of Miami).\nrUM includes:\n\nA research project template. It creates a new RStudio project that has your choice of an analysis.qmd Quarto file or analysis.Rmd R markdown file with tidyverse and conflicted.\nQuarto and R Markdown templates which include they YAML header and start up blocks that load the tidyverse and conflicted packages.\n💥 NEW in Version 2.0.0 (Overproof Rum) 💥 rUM now can make a package project that includes a paper outline as a vignette. rUM can now add an example table and figure to it’s paper shell.\n\n\nGet it now!"
  },
  {
    "objectID": "blog/posts/2024-03-04-creating-quarto-hero.html",
    "href": "blog/posts/2024-03-04-creating-quarto-hero.html",
    "title": "Creating a Quarto website hero panel",
    "section": "",
    "text": "Let’s explore creating a hero panel in Quarto using some raw HTML code and SCSS styling tweaks."
  },
  {
    "objectID": "blog/posts/2024-03-04-creating-quarto-hero.html#what-is-a-hero-panel",
    "href": "blog/posts/2024-03-04-creating-quarto-hero.html#what-is-a-hero-panel",
    "title": "Creating a Quarto website hero panel",
    "section": "What is a hero panel?",
    "text": "What is a hero panel?\nWe’ve all seen them, but may not have known what to call them. They’re the image that sits usually at the top of a home page. They’re designed to immediately draw your eyes to purpose and main message of the site. Hero panels, also referred to as “hero sections” or “hero images”, have evolved from the newspaper printing concept of “above the fold”. When you see a newspaper, you’re immediately drawn to what the editors intended for you to see first: a bold headline and image. The hope is that it compels you to pick it up and read on.\nIn web design, the hero panel is situated “above the fold”, but instead, this format requires our content to be visible without scrolling. We’re going to create a hero panel in a Quarto HTML document. We’ll also use some simple HTML create the initial layout, and then write some SCSS to style our panel and place the text where we want.\n\n\n\n\n\n\nIf you’re interested in learning more about HTML and CSS, a resource that I used and still often reference is W3 Schools. They have free content consisting of bite-sized tutorials to quickly get your feet wet."
  },
  {
    "objectID": "blog/posts/2024-03-04-creating-quarto-hero.html#lets-build",
    "href": "blog/posts/2024-03-04-creating-quarto-hero.html#lets-build",
    "title": "Creating a Quarto website hero panel",
    "section": "Let’s build!",
    "text": "Let’s build!\n\nThis is the hero panel that we will be recreating. Now, I have to give the disclaimer that this is but one of a multitude of ways to achieve the same goal. For me, this was the easiest as it uses minimal HTML and CSS. I’m going to assume that you know what HTML elements and tags are. If you don’t, the short and sweet: elements are the Lego blocks used to build your site or document and tags are the instructions for the browser to know what the element is and where to put it. Tags are identified by their &lt;&gt; brackets surrounding things like &lt;h1&gt; or &lt;img&gt;.\n\nOne small package\nUsing the lorem package can quickly render content to help visualize your layout. Later, you can go back and add what will ultimately be final version. As you can see, it’s that Latin text that is composing the paragraphs below the hero panel. Let’s download it now:\n\ninstall.packages('lorem')\n\n\n\nTime for Quarto\nYou need to have Quarto installed. Download the latest version here if you need it. In RStudio, click File &gt;&gt; New File &gt;&gt; Quarto Document to create a new Quarto file. Select HTML, leave the document untitled, uncheck Editor: Use visual markdown editor, and lastly Create Empty Document.\n\n\n\nModify the YAML header\nLet’s make a few changes in the YAML header. To reproduce our example image, let’s remove the title so that we can place the hero image directly at the top of the document. If you’re using this tutorial for adding hero panel to your Quarto website, leave the title as that area will be occupied by the navigation bar. Adding embed-resources: true creates a standalone HTML file that incorporates necessary images, stylesheets, and JavaScript without relying on external files. This can increase accessibility of the file. Finally, we’re going to create layout-styles.scss for styling and positioning HTML elements, so let’s add that here.\n---\ntitle: \"\"\nformat: \n  html:\n    embed-resources: true\ntheme: layout-styles.scss\n---\n\n\nAdding raw HTML to Quarto\nAgain, for me, this was the simplest method to create a hero panel. Under the YAML, add in this HTML:\n---\ntitle: \"\"\nformat: \n  html:\n    embed-resources: true\ntheme: layout-styles.scss\n---\n\n&lt;!----- start hero panel --------&gt;\n\n```{=html}\n&lt;div class=\"hero-panel column-screen\"&gt;\n  &lt;img class=\"banner\" src=\"banner.jpg\" alt=\"Banner Image\"&gt;\n  &lt;div class=\"banner-text\"&gt;\n    &lt;h1&gt;Learning Quarto Layout Tricks&lt;/h1&gt;\n    &lt;h3&gt;This is the hero panel&lt;/h3&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n```\n\n&lt;!----- end hero panel ----------&gt;\n\n\nBreaking this down, a div element is a container that holds something. Adding one or more classes to a div helps to “find” this element when we add some styles to it. The div class=\"hero-panel column-screen\" is our main container. Within that div, you can see our img and another div holding our text in the h1 and h3 elements. A cool trick that I learned was setting class=\"column-screen\" in Quarto allows the div to occupy the entire viewport! That’s great because it helps make our styling that much easier.\n\n\nStyle it out\nWe just modified the YAML header by pointing to our stylesheet, layout-styles.scss. We’re going to use a SCSS file as opposed to CSS because it will give you a little more freedom later on in your project to write your styling to be more readable, if nothing else.\nCreate layout-styles.scss and let’s add some content. When we write the styles, we can format it in much the same way that we described the aspect of container: we can format the SCSS so that it gives the appearance that our styles are “contained” within other parts of the code. Here, let me show you what I mean:\n// Set up the hero panel\n.hero-panel {\n  // This is required. Think of it like a family unit where they are \"relative\"s\n  position: relative;\n  \n  img.banner {\n    width: 100%;\n    height: 350px;\n  }\n  \n  .banner-text {\n    position: absolute;\n    left: 5%;\n    bottom: 20%;\n    color: white;\n    font-weight: bold;\n  }\n}\nOur main container for our hero panel’s content lives inside the container that we gave the class hero-panel when we wrote the raw HTML. Now if you look at the SCSS that we added, img.banner is written inside of the {} braces for .hero-panel. This is “nesting” where one element’s CSS code can be read by human eyes as existing within another element. In our case, you’ll notice that both img.banner and banner-text are nested inside of .hero-panel.\n\n\n\n\n\n\nTwo quick things: you can identify a class that was written in HTML within the SCSS (or CSS) code because they’ll start with a . like in .hero-panel. Also, img.banner is an image (img) with the class of banner… put it together and you have img.banner for the SCSS.\n\n\n\nThe most difficult concept for me to initially grasp was this idea of position: relative; versus position: absolute;. I bet there will be much more technical and better ways of explaining this talking about the document flow and whatnot, but our main container, .hero-panel has “child” elements inside that we’re using as well. To be less technical about it, what we’re concerned with is that its elements are “related” to it, so we’ll give .hero-panel the position: relative.\nThere are only a few children within our parent container: the image and some text. Referring back to the HTML that we wrote and armed with the understanding that using proper indentation can help visually group elements for readability, our text container was given the class banner-text. Adding the . for our SCSS now, we’re going to give .banner-text the position: absolute. Why? Changing its position to absolute can be thought of this way: a parent can pick up a child and absolutely put that child where they want. For us, we want parent (hero-panel) to move its child (banner-text) where it wants to put it…ugh, where we want to put it. Gosh, I hope that made sense.\nWe’re going to move all of the text inside banner-text starting from the left and moving to the middle 5%. I recommend using percentages as opposed to pixels (px) because this could help when your page is rendered on different size screens. I’m sure now you can guess that bottom: 20%; moves the position of the text elements up 20% from the bottom. You can also use right and top for positioning elements absolutely too, but we just don’t use them that way in our example.\nThe final part is that we want our text to have the color: white; and we want it to stand out a little more, so let’s make font-weight: bold;.\n\n\nIs that it?!\nYou can complete the look by adding a H2 header (using two ## in Quarto) for “Words in Latin…”. Then add a r code block with lorem::ipsum(paragraphs = 3) to fill in the rest of the space.\nViola! That should do it. But be sure to look for other ways to add more styling to your text or image for the hero panel. You can add shadowing effects to the text, you can make the image appear a little darker, or you can even blur the background image if that fits the idea you have. There are endless possibilities and I’m constantly learning and improving.\nTo be honest, I hope that I reread this post in a few months and immediately want to rewrite this to be even clearer or explained better. For now, this is where I am and I hope I was able to share something with you.\nThank you for your time and check back soon!\n\nShare your insights at kylegrealis@icloud.com. Together, we can make our R projects more robust, reproducible, and ready for collaboration!\nHappy coding!\n~Kyle"
  },
  {
    "objectID": "blog/posts/2025-04-22-tailscale.html",
    "href": "blog/posts/2025-04-22-tailscale.html",
    "title": "Home Lab, part 1: Tailscale",
    "section": "",
    "text": "Go check out The “Why?” to understand my intention of this whole self-hosting endeavor!"
  },
  {
    "objectID": "blog/posts/2025-04-22-tailscale.html#automatic-tailscale-exit-node-management-based-on-wi-fi-networks",
    "href": "blog/posts/2025-04-22-tailscale.html#automatic-tailscale-exit-node-management-based-on-wi-fi-networks",
    "title": "Home Lab, part 1: Tailscale",
    "section": "Automatic Tailscale Exit Node Management Based on Wi-Fi Networks",
    "text": "Automatic Tailscale Exit Node Management Based on Wi-Fi Networks\nThis documentation explains how to set up automatic switching of Tailscale exit node configurations based on which Wi-Fi network you’re connected to. On untrusted public networks (coffee shops, airports, hotels), routing all traffic through your secure exit node creates an encrypted tunnel that prevents local eavesdropping and protects your data from potential attackers on the same network. However, this protection comes with trade-offs - all traffic must travel to your exit node first before reaching its destination, which can increase latency and effectively double your data’s journey across the internet. On trusted home networks where security concerns are minimal, disabling exit node routing allows for direct internet access with lower latency and better performance for bandwidth-intensive applications. This automation solution gives you the best of both worlds: automatic protection when you need it and direct connections when you don’t, without requiring manual intervention every time you change networks."
  },
  {
    "objectID": "blog/posts/2025-04-22-tailscale.html#overview",
    "href": "blog/posts/2025-04-22-tailscale.html#overview",
    "title": "Home Lab, part 1: Tailscale",
    "section": "Overview",
    "text": "Overview\nWhen connecting to different Wi-Fi networks, you may want different Tailscale behaviors:\n\nOn trusted home networks: Disable Tailscale exit node (direct internet access)\nOn untrusted networks: Enable Tailscale exit node (route all traffic through your secure server)\n\nThis solution uses NetworkManager’s dispatcher scripts to automatically detect network changes and apply the appropriate Tailscale configuration.\n\n\n\n\n\n\nLinux-tested\n\n\n\nThe solution I’ve described is specifically designed for Linux systems using NetworkManager, so it won’t work directly on macOS or Windows without significant modifications. The core concept (detecting network changes and toggling Tailscale settings) is transferable, but the implementation details, commands, and file locations would be completely different on each operating system. Each would require its own platform-specific implementation of the same idea."
  },
  {
    "objectID": "blog/posts/2025-04-22-tailscale.html#component-1-tailscale-home-script",
    "href": "blog/posts/2025-04-22-tailscale.html#component-1-tailscale-home-script",
    "title": "Home Lab, part 1: Tailscale",
    "section": "Component 1: Tailscale Home Script",
    "text": "Component 1: Tailscale Home Script\nThe first component is a script that disables Tailscale exit node routing when on trusted networks (i.e., home):\n#!/usr/bin/env bash\n\n# tailscale-home.sh - Disable exit node routing\n# Usage: ./tailscale-home.sh\n\necho \"Disabling Tailscale exit node protection...\"\nsudo tailscale up --exit-node=\"\"\n\n# Verify the change\nSTATUS=$(tailscale status)\n\nif echo $STATUS | grep -q \"offers exit node\"; then\n    echo \"✅ Exit node protection disabled! Traffic now routes normally.\"\nelse\n    echo \"❌ Something went wrong. Please check tailscale status.\"\nfi\nThis script: 1. Runs the Tailscale command to disable exit node routing (--exit-node=\"\") 2. Checks if the change was successful 3. Provides feedback on the operation’s success"
  },
  {
    "objectID": "blog/posts/2025-04-22-tailscale.html#component-2-tailscale-protect-script",
    "href": "blog/posts/2025-04-22-tailscale.html#component-2-tailscale-protect-script",
    "title": "Home Lab, part 1: Tailscale",
    "section": "Component 2: Tailscale Protect Script",
    "text": "Component 2: Tailscale Protect Script\nThe second component enables the exit node when on untrusted networks (i.e., coffee shop, hotel, public Wi-Fi).:\n#!/usr/bin/env bash\n\n# tailscale-protect.sh - Route all traffic through Pi5 exit node\n# Usage: ./tailscale-protect.sh\n\n# Set the HOST_IP to match your exit node's Tailscale IP\nHOST_IP=\"100.xxx.xxx.xxx\"\n\necho \"Enabling Tailscale exit node protection...\"\nsudo tailscale up --exit-node=$HOST_IP\n\n# Verify the change\nSTATUS=$(tailscale status)\n\nif echo $STATUS | grep -q \"; exit node;\"; then\n    echo \"✅ Exit node protection enabled! All traffic now routes through your Tailnet.\"\nelse\n    echo \"❌ Something went wrong. Please check tailscale status.\"\nfi\nThis script: 1. Sets the exit node to your host’s Tailscale IP address 2. Verifies the change was successful 3. Provides feedback on the operation’s success"
  },
  {
    "objectID": "blog/posts/2025-04-22-tailscale.html#component-3-networkmanager-dispatcher-script",
    "href": "blog/posts/2025-04-22-tailscale.html#component-3-networkmanager-dispatcher-script",
    "title": "Home Lab, part 1: Tailscale",
    "section": "Component 3: NetworkManager Dispatcher Script",
    "text": "Component 3: NetworkManager Dispatcher Script\nThe third component is a NetworkManager dispatcher script that automatically runs the appropriate script based on which network you’re connected to. A log is created and saved, and you can watch it update in real time. I was able to test this by alternating between my home’s “Go_Canes” secure Wi-Fi and a mobile hotspot. Just be sure to update the KNOWN_NETS line below specific to your case:\n#!/usr/bin/env bash\n# Save this as /etc/NetworkManager/dispatcher.d/99-tailscale-autoswitch\n\n# Make sure we have the required permissions\nif [ \"$(id -u)\" != \"0\" ]; then\n   echo \"This script must be run as root\" \n   exit 1\nfi\n\nINTERFACE=$1\nSTATUS=$2\n\n# Define known networks as array: Change listed names as appropriate\nKNOWN_NETS=(\"Go_Canes\" \"Canes_guest\")\nHOME_SCRIPT=\"$HOME/tailscale-home.sh\"\nPROTECT_SCRIPT=\"$HOME/tailscale-protect.sh\"\nLOG_FILE=\"/var/log/tailscale-autoswitch.log\"\n\nlog() {\n    echo \"$(date): $*\" &gt;&gt; \"$LOG_FILE\"\n}\n\nlog \"Network change detected: Interface=$INTERFACE Status=$STATUS\"\n\n# Dynamically detect WiFi interface name\nWIFI_INTERFACE=$(nmcli -t -f DEVICE,TYPE device | grep \":wifi$\" | cut -d: -f1)\n\n# Only act on wifi connections that are activated\nif [ \"$STATUS\" = \"up\" ] && [ \"$INTERFACE\" = \"$WIFI_INTERFACE\" ]; then\n    # Get the current SSID\n    CURRENT_SSID=$(nmcli -t -f active,ssid dev wifi | grep '^yes' | cut -d: -f2)\n    log \"Connected to SSID: $CURRENT_SSID\"\n\n    # Initialize a flag to check if the network is known\n    KNOWN=false\n\n    for NETWORK in \"${KNOWN_NETS[@]}\"; do\n        if [ \"$CURRENT_SSID\" = \"$NETWORK\" ]; then\n            KNOWN=true\n            break\n        fi\n    done\n\n    if [ \"$KNOWN\" = true ]; then\n        log \"Home network detected: $CURRENT_SSID. Running home script...\"\n        bash \"$HOME_SCRIPT\" &gt;&gt; \"$LOG_FILE\" 2&gt;&1\n    else\n        log \"Unknown network detected: $CURRENT_SSID! Running protect script...\"\n        bash \"$PROTECT_SCRIPT\" &gt;&gt; \"$LOG_FILE\" 2&gt;&1\n    fi\nfi\n\nexit 0\n\nHow the Dispatcher Script Works\n\nPermissions Check: Makes sure the script runs as root\nNetwork Detection: Takes network interface and status from NetworkManager\nKnown Networks List: Maintains an array of trusted Wi-Fi SSIDs\nLogging Function: Creates a detailed log of all operations\nDynamic Interface Detection: Automatically finds the WiFi interface name rather than hardcoding it\nInterface Filtering: Only processes WiFi connections that are coming up\nSSID Extraction: Gets the SSID of the currently connected network\nNetwork Verification: Checks if the current network is in the trusted list\nScript Execution: Runs either the home script or protect script based on network trust\nLogging Results: Records the entire process for troubleshooting"
  },
  {
    "objectID": "blog/posts/2025-04-22-tailscale.html#installation-steps",
    "href": "blog/posts/2025-04-22-tailscale.html#installation-steps",
    "title": "Home Lab, part 1: Tailscale",
    "section": "Installation Steps",
    "text": "Installation Steps\n\nCreate the home and protect scripts:\nnano ~/tailscale-home.sh\nnano ~/tailscale-protect.sh\nMake them executable:\nchmod +x ~/tailscale-home.sh\nchmod +x ~/tailscale-protect.sh\nCreate the NetworkManager dispatcher script:\nsudo nano /etc/NetworkManager/dispatcher.d/99-tailscale-autoswitch\nMake the dispatcher script executable:\nsudo chmod +x /etc/NetworkManager/dispatcher.d/99-tailscale-autoswitch\nCreate a log file:\nsudo touch /var/log/tailscale-autoswitch.log\nsudo chmod 666 /var/log/tailscale-autoswitch.log"
  },
  {
    "objectID": "blog/posts/2025-04-22-tailscale.html#testing",
    "href": "blog/posts/2025-04-22-tailscale.html#testing",
    "title": "Home Lab, part 1: Tailscale",
    "section": "Testing and Monitoring",
    "text": "Testing and Monitoring\nYou can monitor the script’s operation by viewing the log file:\ntail -f /var/log/tailscale-autoswitch.log\nExample log output showing automatic switching between networks:\nMon Apr 21 22:58:43 CDT 2025: Network change detected: Interface=wlp0s20f3 Status=up\nMon Apr 21 22:58:43 CDT 2025: Connected to SSID: Go_Canes\nMon Apr 21 22:58:43 CDT 2025: Home network detected: Go_Canes. Running home script...\nDisabling Tailscale exit node protection...\n✅ Exit node protection disabled! Traffic now routes normally.\n\nMon Apr 21 23:03:34 CDT 2025: Network change detected: Interface=wlp0s20f3 Status=up\nMon Apr 21 23:03:34 CDT 2025: Connected to SSID: Alexa?s iPhone\nMon Apr 21 23:03:34 CDT 2025: Unknown network detected: Alexa?s iPhone! Running protect script...\nEnabling Tailscale exit node protection...\n✅ Exit node protection enabled! All traffic now routes through your Tailnet.\n\nMon Apr 21 23:04:00 CDT 2025: Network change detected: Interface=wlp0s20f3 Status=up\nMon Apr 21 23:04:00 CDT 2025: Connected to SSID: Go_Canes\nMon Apr 21 23:04:00 CDT 2025: Home network detected: Go_Canes. Running home script...\nDisabling Tailscale exit node protection...\n✅ Exit node protection disabled! Traffic now routes normally.\nThe log shows:\n\nNetwork changes detected\nWhich SSID you’re connecting to\nWhether the network is trusted or untrusted\nWhich script was executed\nThe result of the operation"
  },
  {
    "objectID": "blog/posts/2025-04-22-tailscale.html#customization",
    "href": "blog/posts/2025-04-22-tailscale.html#customization",
    "title": "Home Lab, part 1: Tailscale",
    "section": "Customization",
    "text": "Customization\n\nAdd More Trusted Networks: Modify the KNOWN_NETS array to include additional trusted networks:\nKNOWN_NETS=(\"Go_Canes\" \"Canes_guest\" \"Work_WiFi\" \"Parents_Home\")\nLog Rotation: To prevent the log file from growing too large:\n# Add near the top of the script\nif [ -f \"$LOG_FILE\" ] && [ $(stat -c%s \"$LOG_FILE\") -gt 1048576 ]; then\n    mv \"$LOG_FILE\" \"$LOG_FILE.old\"\nfi\nExit Node Connectivity Check: Add to the protect script to check if your exit node is online:\n# Add to tailscale-protect.sh before enabling the exit node\nping -c 1 $PI5_IP &gt; /dev/null 2&gt;&1\nif [ $? -ne 0 ]; then\n    echo \"⚠️ Warning: Exit node appears to be offline!\"\nfi\nModify Log Location: Change the LOG_FILE variable to use a different log file location."
  },
  {
    "objectID": "blog/posts/2025-04-22-tailscale.html#understanding-networkmanager-events",
    "href": "blog/posts/2025-04-22-tailscale.html#understanding-networkmanager-events",
    "title": "Home Lab, part 1: Tailscale",
    "section": "Understanding NetworkManager Events",
    "text": "Understanding NetworkManager Events\nNetworkManager triggers multiple events during a network connection:\n\nNetwork down event\nDHCP4 configuration (IPv4)\nInterface up event (this is when our script runs)\nDNS configuration\nDHCP6 configuration (IPv6)\n\nOur script specifically targets the “up” event to ensure the network is ready before applying Tailscale configurations."
  },
  {
    "objectID": "blog/posts/2025-04-22-tailscale.html#why-dynamic-interface-detection-matters",
    "href": "blog/posts/2025-04-22-tailscale.html#why-dynamic-interface-detection-matters",
    "title": "Home Lab, part 1: Tailscale",
    "section": "Why Dynamic Interface Detection Matters",
    "text": "Why Dynamic Interface Detection Matters\nThe script uses dynamic interface detection instead of hardcoding the WiFi interface name. This future-proofs your setup because WiFi interface names can change when:\n\nYou replace or add network hardware\nYou upgrade your kernel or distribution\nYour system’s PCI enumeration changes\nYou connect an additional WiFi adapter (like a USB dongle)\n\nBy detecting the interface dynamically, the script will continue working even if these changes occur."
  },
  {
    "objectID": "blog/posts/2025-04-22-tailscale.html#troubleshooting",
    "href": "blog/posts/2025-04-22-tailscale.html#troubleshooting",
    "title": "Home Lab, part 1: Tailscale",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nIf the script isn’t working as expected:\n\nCheck the log file for errors:\ntail -f /var/log/tailscale-autoswitch.log\nVerify the script is executable:\nls -l /etc/NetworkManager/dispatcher.d/99-tailscale-autoswitch\nTest the scripts manually:\nsudo bash $HOME/tailscale-home.sh\nsudo bash $HOME/tailscale-protect.sh\nCheck your current WiFi interface name:\nnmcli device status | grep wifi\nTest interface detection:\nnmcli -t -f DEVICE,TYPE device | grep \":wifi$\" | cut -d: -f1\n\n\nThis automation brings peace of mind to your network security journey. With Tailscale now intelligently routing your traffic based on your trusted network list, you’ve created a system that protects you automatically without requiring constant manual intervention. Now that your secure self-hosted network intelligently adapts to your environment, it’s time to expand its capabilities by adding services that fulfill the core goals of this project. The foundation is solid—let’s build something amazing on top of it!\n\nShare your insights at kylegrealis@icloud.com. Together, we can make our R (and self-hosted) projects more robust, reproducible, and ready for collaboration!\nHappy coding!\n~Kyle"
  },
  {
    "objectID": "blog/posts/2025-04-22-tailscale.html#additional-resources",
    "href": "blog/posts/2025-04-22-tailscale.html#additional-resources",
    "title": "Home Lab, part 1: Tailscale",
    "section": "Additional Resources",
    "text": "Additional Resources\n\ntailscale"
  },
  {
    "objectID": "blog/posts/2024-05-22-beginner-git-team-workflow.html#using-a-collaborative-git-workflow",
    "href": "blog/posts/2024-05-22-beginner-git-team-workflow.html#using-a-collaborative-git-workflow",
    "title": "Quick git for beginners… yeah, that’s me!",
    "section": "Using a collaborative git workflow",
    "text": "Using a collaborative git workflow\n\n\n\n\n\n\nSince this was originally written for biostatistics students at the University of Miami, some of the steps will be referring to options as they exist using RStudio as the IDE. While I use RStudio and VS Code, slight adaptations to these instructions will need to be made by you in order to fit your IDE’s layout and options. 😊\n\n\n\nWhen collaborating with a team (or even when creating and using personal data science project), these steps can help avoid potential conflicts and other issues that arise while using git. Please keep in mind that this list isn’t expansive nor does it get into nuanced details. I’ll leave those technical bits for instructionals that are better suited to handle complicated matters. What this post is, though, is a starter guide sprinkled with bits of anecdotal suggestions that I hope can keep you focused on the task at hand: being an awesome data scientist!\n\nNavigate to your project in the IDE of your choice (RStudio, Posit Cloud, etc.)\nCheck that you are on the main branch by inspecting the git pane in RStudio or typing git branch into the terminal. For peace of mind, and to save complexity, the remainder of this post will not be referencing terminal commands. To learn more, see the git documentation for fantastic instructions.\nPull from the repository into the main branch. This ensures that you have the latest updates to the main version of the project. Sometimes you may not have been informed of changes added to main since your last time using the project. Pulling into main at the start and before working on anything can help avoid issues. Picture a tree– the main branch is the trunk and there are one or more offshoot branches that make the tree. Our git tree (the project) is slightly different though in that sometimes our branches merge into the trunk as the tree ages making the tree better; some branches are bad or we find that they serve no greater purpose and need to be pruned so the tree doesn’t die. Keep the tree strong and don’t hack away at the main branch.\nIF you are creating a new feature, create a new branch. For example, if I were to write a new script to clean a dataset, I could create a new branch named data-clean-kg to tell other members what I’m doing. I like to add my initials at the end of the branch name to help my team members know who did what. It’s a convention I use, but adopt your own. Keep in mind that it’s best to make a branch for one feature, work on that feature, then continue these steps. It’s not always recommended to make a branch and work on multiple aspects of the project on the same branch. Back to the tree analogy, if too much weight rests on one branch (too many features relying on one branch), when the branch breaks so does the tree. Remember, these are suggestions and not hard-and-fast rules.\nOne feature per branch. This deserves its own line. Again, a feature doesn’t mean one script file. A feature is a new or edited aspect of the current project. Sometimes a new file must be sourced into another, so you can’t help but rendering changes to more than one piece of the overall project. What I’m referring to is this: in BST692 at the University of Miami, you’ll be constructing many different types of machine learning models… don’t write your logistic model on the same branch as your LASSO and KNN and random forest. Make sense? Instead make a branch lasso-kg or knn-kg or get-logistic-metrics-kg.\nstage & commit your changes and add a short but descriptive message. I tend to write commit messages like these: initial commit: data cleaning script or edit: added new dataset and joined with original or feat: added Shiny dropdown (with “feat” being feature). Short and descriptive messages are helpful and your future self (or team members) will thank you if the project needs to roll back to a working state. Things break, mistakes happen, but documenting along the way will quickly get your project up and running again. So, write some code, stage and commit with a message, and get into the zen coding flow state.\npush your commit to the project repository. Get into the habit of pushing your commits from your branch to the project repository. This will not add them to the main branch if you are working on a remote branch… as you should be if you completed Step 4. Do this before lunch, do it before that meeting, do it while you find your brain drift away from what you’re working on, but do it before you leave the office for the day! The beauty of using a tool like GitHub is that I’ll be upset if I brick my computer and lose my hard drive, but not as upset if my monthslong machine learning project can be restored onto another machine from the cloud.\nOpen a pull request in GitHub and add team members to review. Those reviewers will see your changes, have the ability to approve or add comments or both, and provide constructive feedback. This is done openly so others can see what’s happening within the project. For example, I request reviews from the main authors when working on the rUM package and we provide feedback if a new edit or feature is good, needs improvement, or is a bad idea. Finally, when another collaborator approves…\nYOU MADE THE BRANCH, YOU MERGE THE CHANGES & DELETE THE BRANCH! You should not merge changes that you did not personally create and you definitely should not delete a branch you did not create. This is very important for collaborative workflows. I’ve had my proverbial hand slapped for approving a pull request, merging it into the main project, and deleting the branch… all from someone else’s work. This should always be regarded as a big “No, don’t do it!!!” The feature or branch’s author should be the only one to literally push the buttons to merge (again, once approved) and delete the branch.\n\n\nI hope this little guide was helpful and provides insight on how to work as a team. I’m a beginner and always find ways to improve my workflow. I tend to test things on my own before doing them in the collaborative setting. Working with other people affords the opportunity to communicate effectively and efficiently. Use those same concepts when working with git: write commit messages that aren’t update done and rather edit: added forest plot for subset predictors. Lastly, this post will probably be updated as I find better ways.\nShare your insights at kylegrealis@icloud.com. Together, we can make our R projects more robust, reproducible, and ready for collaboration!\nHappy coding!\n~Kyle"
  },
  {
    "objectID": "blog/posts/2025-04-14-github-actions-email.html",
    "href": "blog/posts/2025-04-14-github-actions-email.html",
    "title": "Automating a GitHub Actions status email",
    "section": "",
    "text": "Using automation with GitHub Actions is a great way to simplify project updates. This example will demonstrate how to connect an iCloud account to your repository and receive automated emails after a workflow job has completed."
  },
  {
    "objectID": "blog/posts/2025-04-14-github-actions-email.html#purpose",
    "href": "blog/posts/2025-04-14-github-actions-email.html#purpose",
    "title": "Automating a GitHub Actions status email",
    "section": "",
    "text": "Using automation with GitHub Actions is a great way to simplify project updates. This example will demonstrate how to connect an iCloud account to your repository and receive automated emails after a workflow job has completed."
  },
  {
    "objectID": "blog/posts/2025-04-14-github-actions-email.html#requirements",
    "href": "blog/posts/2025-04-14-github-actions-email.html#requirements",
    "title": "Automating a GitHub Actions status email",
    "section": "Requirements",
    "text": "Requirements\n\nGitHub repository\niCloud email account\nEnabling an app-specific password in iCloud\nA GitHub actions workflow file"
  },
  {
    "objectID": "blog/posts/2025-04-14-github-actions-email.html#steps",
    "href": "blog/posts/2025-04-14-github-actions-email.html#steps",
    "title": "Automating a GitHub Actions status email",
    "section": "Steps",
    "text": "Steps\n\nWhile this demonstration uses iCloud, adapting Ravgeet Dhillon’s blog post to suit your needs should be fairly easy.\n\n\nPart 1: iCloud\n\nLogin to your iCloud account and navigate to your icon in the upper right of the screen. Select “Manage Apple Account” from the list of options:\n\n\n\nProceed to login and complete the two-factor authorization, if enabled.\nSelect “App-Specific Passwords” from the tile options.\n\n\n\nSelect the plus symbol (+).\n\n\n\nAdd a name for your passowrd.\n\n\n\nComplete the verification.\n\n\n\nSave and copy this password immediately! You cannot recover or see it again once you select “Done”.\n\n\n\n\nPart 2: GitHub\n\nNavigate to the repository where you will be adding the GitHub Action. From the top bar, select “Settings”. On the left side, select “Secrets and variables” then “Actions”.\n\n\n\nFollow this format. Store the iCloud app-specific password you created above in a “New repository secret” named EMAIL_PASSWORD. Add your iCloud email address to a “New repository secret” named EMAIL_USERNAME.\n\n\n\nIf you use different secret names, you will need to modify the YAML file provided below to match.\n\n\n\nPart 3: Workflow file\n\nCreate a new file in your repository. As an example, the file is named_build.yml and will be stored in this path from the root of your project .github/workflows. If you don’t currently have those directories, you can enter the following command in your terminal:\n\nmkdir -p .github/workflows\ntouch _build.yml\nUsing the -p flag, this will make directory or directories if they do not currently exist and create (touch) the new file. Alternatively, you can use the GUI tools if that’s easier.\n\nOpen _build.yml and paste in this content:\n\nname: Build\n\non:\n  push:\n    branches: main\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Hello World\n        run: echo Hello, world!\n\n      - name: Send status email via iCloud\n        if: always()\n        uses: dawidd6/action-send-mail@v3\n        with:\n          server_address: smtp.mail.me.com\n          server_port: 587  # iCloud SMTP requires TLS (STARTTLS)\n          secure: false  # This must be false for STARTTLS to work\n          username: ${{ secrets.EMAIL_USERNAME }}\n          password: ${{ secrets.EMAIL_PASSWORD }}\n          subject: \"${{ github.repository }} workflow: ${{ job.status }}\"\n          # github.job will be lowercase of the `name` value in line 1. Here, \"build\".\n          # github.workflow will will match the `name` value in line 1\n          # github.repository is the repository name: kyleGrealis/my-repo-name\n          # job.status: \"success\" or \"failure\" \n          body:  |\n            ${{ github.workflow }} of ${{ github.repository }}\n\n            Job: ${{ github.job }}\n            Status: ${{ job.status }}\"\n          to: \"kylegrealis@icloud.com\"\n          from: \"From my GitHub Actions Bot\"\nThough this file may initially appear as overwhelming or complicated, it uses a relatively straight-forward key:value syntax:\n\nName the workflow “Build”. This will appear under the “Actions” tab and then the left side of the screen for your GitHub repository.\n\n\n\nThe on: section states that this action will run on each project push on the main branch. To add other branches, move main to the next line with one more indent and a dash:\n\non:\n  push:\n    branches:\n      - main\n      - dev\n\nThe jobs section declares what is to happen and how it is to happen. The explanations are beyond the scope of this, so explore GitHub documentation for more information.\nThere will be two main jobs that will run:\n\n\nEcho “Hello World” to the Actions console.\nSend a status email\n\n\nThe email with always() be sent, regardlesss if the job was a success or failure.\nBe sure not to alter the server or secure lines or risk unexpected consequences or workflow failure.\nThe username and password are self-explanatory. The secret names you provided in Part 2 must match the names inside the braces (e.g., secrets.EMAIL_USERNAME) to correctly provide those values. This format prevents leaking your personal information to others, like the author is doing here.\nThe subject and body can be modfied with whatever information you desire. Use body: | then indent to create a multiple line & custom structure to the email. A successful run will return this with your own information included:\n\n\n\nSave, commit, add a message, and push this file to your repository. You should quickly receive a status email if everything was done correctly. You can view the real-time workflow by navigating to the “Actions” tab in your repository, select the name of your action (i.e., “Build”), and select the commit message you gave. Using the image from above (and repeated here), the commit message was “updated”. Selecting that will give you an easy interface to select and check the logs. The logs provide a good amount of detail to help the debugging process if something went wrong."
  },
  {
    "objectID": "blog/posts/2025-04-14-github-actions-email.html#conclusion",
    "href": "blog/posts/2025-04-14-github-actions-email.html#conclusion",
    "title": "Automating a GitHub Actions status email",
    "section": "Conclusion",
    "text": "Conclusion\nGitHub Actions can really enhance your data science workflow by offloading manual tasks to action-specific or chronological jobs. Integrating them is quite easy and only requires about 30 minutes of reading and testing before you will be able to adapt this simple example to meet your needs.\nShare your insights at kylegrealis@icloud.com. Together, we can make our R projects more robust, reproducible, and ready for collaboration!\nHappy coding!\n~Kyle"
  },
  {
    "objectID": "blog/posts/2024-05-14-using-docker-python-r.html",
    "href": "blog/posts/2024-05-14-using-docker-python-r.html",
    "title": "Setting Up a Python or R Project with Docker",
    "section": "",
    "text": "This guide will walk you through the process of setting up a Python or R project with Docker. This is particularly useful for data science projects where you need to ensure that your code runs in a consistent environment."
  },
  {
    "objectID": "blog/posts/2024-05-14-using-docker-python-r.html#step-1-organize-your-python-scripts",
    "href": "blog/posts/2024-05-14-using-docker-python-r.html#step-1-organize-your-python-scripts",
    "title": "Setting Up a Python or R Project with Docker",
    "section": "Step 1: Organize Your Python Scripts",
    "text": "Step 1: Organize Your Python Scripts\nOrganize your Python scripts so that each script is responsible for a specific part of your project. For example:\n\nimport_and_clean.py: Responsible for importing and cleaning your data.\nexplore.py: Responsible for exploring your data (e.g., generating descriptive statistics, creating visualizations).\nmodeling.py: Responsible for building and evaluating your models.\ncreate_api.py: Responsible for creating an API for your model (if applicable)."
  },
  {
    "objectID": "blog/posts/2024-05-14-using-docker-python-r.html#step-2-create-a-main-script",
    "href": "blog/posts/2024-05-14-using-docker-python-r.html#step-2-create-a-main-script",
    "title": "Setting Up a Python or R Project with Docker",
    "section": "Step 2: Create a Main Script",
    "text": "Step 2: Create a Main Script\nCreate a main script that imports and runs the functions from your other scripts in the necessary order. For example:\n\n# main.py\n\n# Import the functions from your other scripts\nfrom import_and_clean import import_and_clean\nfrom explore import explore\nfrom modeling import modeling\nfrom create_api import create_api\n\ndef main():\n    # Call the functions in the necessary order\n    import_and_clean()\n    explore()\n    modeling()\n    create_api()\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "blog/posts/2024-05-14-using-docker-python-r.html#step-3-create-a-requirements.txt-file",
    "href": "blog/posts/2024-05-14-using-docker-python-r.html#step-3-create-a-requirements.txt-file",
    "title": "Setting Up a Python or R Project with Docker",
    "section": "Step 3: Create a requirements.txt File",
    "text": "Step 3: Create a requirements.txt File\nCreate a requirements.txt file that lists the Python packages your project depends on. You can generate it by running pip freeze &gt; requirements.txt in your virtual environment."
  },
  {
    "objectID": "blog/posts/2024-05-14-using-docker-python-r.html#step-4-create-a-dockerfile",
    "href": "blog/posts/2024-05-14-using-docker-python-r.html#step-4-create-a-dockerfile",
    "title": "Setting Up a Python or R Project with Docker",
    "section": "Step 4: Create a Dockerfile",
    "text": "Step 4: Create a Dockerfile\nCreate a Dockerfile that sets up the environment for your project. Here’s an example:\n# Use an official Python runtime as a parent image\nFROM python:3.12-slim-buster\n\n# Set the working directory in the container to /app\nWORKDIR /app\n\n# Add the current directory contents into the container at /app\nADD . /app\n\n# Install any needed packages specified in requirements.txt\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Run main.py when the container launches\nCMD [\"python\", \"main.py\"]"
  },
  {
    "objectID": "blog/posts/2024-05-14-using-docker-python-r.html#step-5-build-and-run-your-docker-container",
    "href": "blog/posts/2024-05-14-using-docker-python-r.html#step-5-build-and-run-your-docker-container",
    "title": "Setting Up a Python or R Project with Docker",
    "section": "Step 5: Build and Run Your Docker Container",
    "text": "Step 5: Build and Run Your Docker Container\nTo build the Docker image, run the following command in your project directory (the same directory where the Dockerfile is located):\nTo run the Docker container, run the following command:\ndocker build -t your-image-name .\ndocker run your-image-name\nThis will run your Python script in a Docker container with an environment that matches the one specified in your Dockerfile."
  },
  {
    "objectID": "blog/posts/2024-05-14-using-docker-python-r.html#step-6-if-building-a-project-in-r",
    "href": "blog/posts/2024-05-14-using-docker-python-r.html#step-6-if-building-a-project-in-r",
    "title": "Setting Up a Python or R Project with Docker",
    "section": "Step 6: If Building a project in R",
    "text": "Step 6: If Building a project in R\n\nCreate a Dockerfile\n\nDocker Base Image: Instead of using a Python base image, you’d use an R base image. For example, you might use FROM r-base:4.1.0 to use R version 4.1.0.\nInstalling Packages: Instead of using a requirements.txt file and pip install, you’d install R packages using the install.packages() function in R. You can do this directly in your Dockerfile. For example:\n\nRUN R -e \"install.packages(c('dplyr', 'ggplot2'), repos='http://cran.rstudio.com/')\"\n\nRunning Your Script: Instead of running a Python script, you’d run an R script. For example:\n\nCMD [\"Rscript\", \"your_script.R\"]\nHere’s what a full Dockerfile might look like for an R project:\n# Use an official R runtime as a parent image\nFROM r-base:4.4.0\n\n# Set the working directory in the container to /app\nWORKDIR /app\n\n# Add the current directory contents into the container at /app\nADD . /app\n\n# Install any needed packages\nRUN R -e \"install.packages(c('dplyr', 'ggplot2'), repos='http://cran.rstudio.com/')\"\n\n# Run your_script.R when the container launches\nCMD [\"Rscript\", \"your_script.R\"]\nAs with the Python example, if you have multiple R scripts that need to be run in a specific order, you can create a main R script that sources and runs your other scripts in the necessary order, and call that script in the CMD line.\nShare your insights at kylegrealis@icloud.com. Together, we can make our R projects more robust, reproducible, and ready for collaboration!\nHappy coding!\n~Kyle"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "My Blog",
    "section": "",
    "text": "Home Lab, part 1: Tailscale\n\n\n\nhomelab\n\nselfhosting\n\nlinux\n\n\n\n\n\n\n\nKyle Grealis\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nHome Lab, part 0: The “Why”?\n\n\n\nhomelab\n\nselfhosting\n\nlinux\n\n\n\n\n\n\n\nKyle Grealis\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nUPDATE: Collaborative Git Workflow\n\n\n\nData science\n\nR\n\nGitHub\n\n\n\n\n\n\n\nKyle Grealis\n\n\nApr 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomating a GitHub Actions status email\n\n\n\nData science\n\nR\n\nGitHub\n\n\n\n\n\n\n\nKyle Grealis\n\n\nApr 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nrenv Project Setup and Sharing Guide\n\n\n\nData science\n\nR\n\nrenv\n\n\n\n\n\n\n\nKyle Grealis\n\n\nAug 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuick git for beginners… yeah, that’s me!\n\n\n\nData science\n\nR\n\ngit\n\n\n\n\n\n\n\nKyle Grealis\n\n\nMay 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting Up a Python or R Project with Docker\n\n\n\nPython\n\nR\n\nDocker\n\nData science\n\n\n\n\n\n\n\nKyle Grealis\n\n\nMay 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning in Python - KNN\n\n\n\nPython\n\nQuarto\n\nMachine Learning\n\n\n\n\n\n\n\nKyle Grealis\n\n\nApr 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a Quarto website hero panel\n\n\n\nR\n\nQuarto\n\nSCSS\n\n\n\n\n\n\n\nKyle Grealis\n\n\nMar 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to scrape Google Scholar using {httr2}\n\n\n\nR\n\nhttr2\n\nwebscraping\n\n\n\n\n\n\n\nKyle Grealis\n\n\nFeb 15, 2024\n\n\n\n\n\n\nNo matching items"
  }
]