{
  "hash": "5644080e18acaf79d00569a56cfdd2ec",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Machine Learning in Python - KNN\"\nimage: /static/images/posts/knn-gist.png\nauthor: \"Kyle Grealis\"\ndate: April 19, 2024\ncategories:\n  - Python\n  - Quarto\n  - Machine Learning\nformat: \n  html:\n    embed-resources: true\n    code-fold: false\n    code-copy: true\nexecute:\n  warning: false\n  message: false\n  eval: false\n---\n\n\n\n\n## What is KNN? \n\nIn this post, I'm sharing the code that was created from following Kirill Eremenko and the SuperDataScience Team's \"Machine Learning A-Z\" course on [Udemy](https://www.udemy.com/share/101Wci3@tAEY1lIEWWOQUxgTL4Ik8e59A6uu7QPTXm_rtWFphiUAplRw_mYHYKk1ACy8OBd_Kw==/). \n\nThe prediction scenario is this: which demographic would social networking marketing ads affect best? We work for a car dealership and have data regarding consumers' age and estimated salary. To where should marketing efforts be aimed as we try to predict which consumers will purchase our newest & best SUV model? \n\n----\n\n\nEuclidean Distance between two points: $\\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}$\n\n*k*-Nearest Neighbors (KNN) is machine learning technique used to classify a new data point to a nearby cluster. We will set up our algorithm to calculate the Euclidean distances from our new data point to existing data points. Then, using the predetermined *k* number of nearest neighbors (we'll be using 5 neighbors), assign that new point to the closest cluster with at least three like $>k/2$ neighbors. \n\nTo think of it in really simple terms, all of our existing customers (points) are scattered in the $(x, y)$ space. Our new customer has $x$ age and $y$ salary, so we'll plot this onto our existing grid. Then we draw circles around the new customer until we hit the closest existing point -- that's one \"neighbor\". We repeat the process until we have our chosen number of 5. How many of the neighbors purchased the SUV and how many did not? Whichever group has more, that's what we're going to predict the new customer would do too!\n\n![*Courtesy of [DataCamp](https://www.datacamp.com)*](images/knn-gist.png)\n\n----\n\n## Import libraries\n\n::: {#f489d62b .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\n```\n:::\n\n\n## Import dataset\n\n::: {#c2907fce .cell execution_count=2}\n``` {.python .cell-code}\ndataset = pd.read_csv('data/Social_Network_Ads.csv')\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n```\n:::\n\n\n----\n\n## Splitting the dataset to training & testing\n\n::: {#a1e45277 .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n  X, y,\n  test_size=0.25,\n  random_state=0\n)\n```\n:::\n\n\n## Feature scaling\n\n::: {#7dd5a65b .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\n```\n:::\n\n\n````{markdown}\n```\n# array([[-1.455..., -0.784...],\n#        [ 2.067...,  1.372...],\n#        [-0.253..., -0.309...],\n#        ...,\n#        [-0.253..., -0.309...],\n#        [ 2.067..., -1.113...],\n#        [-1.455..., -0.309...]])\n```\n````\n\n\n\n\n\n----\n\n## Train & fit the KNN model\n\n::: {.callout-tip}\nTo learn the more technical details of `sklearn`'s classes and functions, checkout the [`sklearn` API Reference](https://scikit-learn.org/1.4/modules/classes.html).\n:::\n\n::: {#8af84202 .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(\n  n_neighbors=5,  # default\n  p=2,            # euclidean distance; default \n  metric='minkowski'\n)\n```\n:::\n\n\n::: {#9a6e7300 .cell execution_count=6}\n``` {.python .cell-code}\nclassifier.fit(X_train, y_train)\n```\n:::\n\n\n![](images/knn-classifier.png)\n\n\n----\n\n## Predicting a new result\n\n30y/o $87k/yr -- first observation of X_test\n\n::: {#a17deb43 .cell execution_count=7}\n``` {.python .cell-code}\nperson = X_test[[0]]\nsingle_pred = classifier.predict(person)\nsingle_prob = classifier.predict_proba(person)\nprint('1=\"Yes\", 0=\"No\"\\n')\nprint(f'Single prediction for 30 y/o earning $87k/yr: {single_pred[0]} at a probability of {single_prob[0][0].round(3)}')\n```\n:::\n\n\n````{markdown}\n```\n# 1=\"Yes\", 0=\"No\"\n#\n# Single prediction for 30 y/o earning $87k/yr: 0 at a \n# probability of 0.8\n```\n````\n\n\n\n\n## Predicting the test set results\n\n::: {#ca0043b4 .cell execution_count=8}\n``` {.python .cell-code}\ny_pred = classifier.predict(X_test)\n```\n:::\n\n\n----\n\n## Creating the confusion matrix\n\n::: {#b7d37f78 .cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nprint(confusion_matrix(y_test, y_pred))\nprint(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n```\n:::\n\n\n````{markdown}\n```\n# [[64  4]\n#  [ 3 29]]\n# Accuracy: 0.93\n```\n````\n\n\n\n\n----\n\n## Visualizing the training set results\n\n::: {.callout-warning}\nThese next two code chunks will take a while. The KNN algorithm is already compute-expensive and we're adding to the heavy lifting by creating a grid of many values to be calculated. The final result is two plots with a visual mapping of our decision boundary **and** our training & predicted data points appearing as the dots within the field.\n:::\n\n::: {#f4c1ae13 .cell execution_count=10}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nX_set, y_set = sc.inverse_transform(X_train), y_train\nX1, X2 = np.meshgrid(\n  np.arange(\n    start = X_set[:, 0].min() - 10, \n    stop = X_set[:, 0].max() + 10, \n    step = 0.25\n  ),\n  np.arange(\n    start = X_set[:, 1].min() - 1000, \n    stop = X_set[:, 1].max() + 1000, \n    step = 0.25\n  )\n)\nplt.contourf(\n  X1, X2, \n  classifier.predict(\n    sc.transform(np.array([X1.ravel(), X2.ravel()]).T)\n  ).reshape(X1.shape),\n  alpha = 0.75, \n  cmap = ListedColormap(('red', 'green'))\n)\n\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\n\nfor i, j in enumerate(np.unique(y_set)):\n  plt.scatter(\n    X_set[y_set == j, 0], \n    X_set[y_set == j, 1], \n    c = ListedColormap(('red', 'green'))(i), \n    label = j\n  )\n\nplt.title('KNN Regression (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n```\n:::\n\n\n![KNN training set results](images/knn-training.png)\n\n## Visualizing the test set results\n\n::: {#d827f550 .cell execution_count=11}\n``` {.python .cell-code}\nX_set, y_set = sc.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(\n  np.arange(\n    start = X_set[:, 0].min() - 10, \n    stop = X_set[:, 0].max() + 10, \n    step = 0.25\n  ),\n  np.arange(\n    start = X_set[:, 1].min() - 1000, \n    stop = X_set[:, 1].max() + 1000, \n    step = 0.25\n  )\n)\nplt.contourf(\n  X1, X2, \n  classifier.predict(\n    sc.transform(np.array([X1.ravel(), X2.ravel()]).T)\n  ).reshape(X1.shape),\n  alpha = 0.75, \n  cmap = ListedColormap(('red', 'green'))\n)\n\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\n\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(\n      X_set[y_set == j, 0], \n      X_set[y_set == j, 1], \n      c = ListedColormap(('red', 'green'))(i), \n      label = j\n    )\n\nplt.title('KNN Regression (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n```\n:::\n\n\n![KNN testing set results](images/knn-testing.png)\n\n\nShare your insights at [kylegrealis@icloud.com](mailto:kylegrealis@icloud.com). Together, we can make our R projects more robust, reproducible, and ready for collaboration!\n\n[Happy coding!]{style='font-size: 1.5rem;'}\n\n[~Kyle]{style='font-size: 1.5rem; font-weight: bold'}\n\n",
    "supporting": [
      "2024-04-19-KNN-in-python_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}